{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "# Keras libraries for CNN\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = pickle.load(open(\"train_images_512.pk\",'rb'), encoding='bytes')\n",
    "train_labels = pickle.load(open(\"train_labels_512.pk\",'rb'), encoding='bytes')\n",
    "test_imgs = pickle.load(open(\"test_images_512.pk\",'rb'), encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_pk(filename):\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         return pickle.load(f, encoding='bytes')\n",
    "\n",
    "# def load_X(filename):\n",
    "#     X = load_pk(filename)\n",
    "#     assert X.size(2) == X.size(3) == SIZE['raw']\n",
    "#     return SCALE * (X[:, :, :, :] + SHIFT) #got rid of the :1 here on the second axis, transformations wanted to see all 3 values\n",
    "\n",
    "\n",
    "# def load_y(filename):\n",
    "#     y = load_pk(filename)\n",
    "#     return y.type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_imgs = load_X(\"train_images_512.pk\")\n",
    "# train_labels = load_y(\"train_labels_512.pk\")\n",
    "# test_imgs = load_X(\"test_images_512.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 2. Define constants ====================\n",
    "\n",
    "# Verbose levels\n",
    "VERBOSE = {\n",
    "    'progress': True,  # Prints progress along the way\n",
    "    'augmenting': False,  # Notify whenever an image transformation goes wrong\n",
    "    'verify': False,  # Shows an image of representative transformed images\n",
    "}\n",
    "\n",
    "# Pathnames and filenames\n",
    "DIR = {\n",
    "    'data': os.path.join('..', 'data'),\n",
    "    'images': os.path.join('..', 'images')\n",
    "}\n",
    "FILENAMES = {\n",
    "    'X_train': os.path.join(DIR['data'], 'train_images_512.pk'),\n",
    "    'y_train': os.path.join(DIR['data'], 'train_labels_512.pk'),\n",
    "    'X_test': os.path.join(DIR['data'], 'test_images_512.pk'),\n",
    "}\n",
    "\n",
    "# Parameters to load raw data\n",
    "SHIFT, SCALE = 1, 127.5  # data = (data + 1) * 127.5\n",
    "SIZE = {'raw': 512, 'final': 256}  # size of raw data and data after downsampling\n",
    "\n",
    "# Relative amount of data used for training vs validation\n",
    "TRAIN_VALID_RATIO = 2 / 1  # i.e. ratio of training data : validation data = 2:1\n",
    "\n",
    "# Parameters for data augmentation\n",
    "N_TRAIN = 1000  # Train using n = 1000\n",
    "POS_NEG_RATIO = 1 / 1  # positive:negative samples in augmented training set = 1:1\n",
    "# (may be backwards)\n",
    "\n",
    "# Parameters for data augmentation\n",
    "TRANSFORMATIONS = torchvision.transforms.Compose([\n",
    "    # Horizontal flip with probability 50%\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # Rotate by up to 15 degrees either way\n",
    "    # Translate by ± 2% of image size\n",
    "    # Expand by up to 15%\n",
    "    torchvision.transforms.RandomAffine(degrees=15, translate=(0.02, 0.02), scale=(1, 1.15)),  # rotation +\n",
    "    # Adjust contrast and brightness by up to 40% and 30% respectively\n",
    "    torchvision.transforms.ColorJitter(contrast=0.4, brightness=0.3),\n",
    "    # Should randomly add black boxes, but doesn't seem to work\n",
    "    torchvision.transforms.RandomErasing(p=0.3, scale=(0.01, 0.03), ratio=(1 / 3, 3)),\n",
    "    # Convert back to torch.tensor\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment training data\n",
    "def augment(X_in, y_in, transform_fn):\n",
    "    # Helper to transform a specific image\n",
    "    # Continues to try for each image until it is successful (takes 1-3 tries)\n",
    "    def transform(im_):\n",
    "        while True:\n",
    "            try:\n",
    "                return transform_fn(im_)\n",
    "            except AttributeError:\n",
    "                verbose_print(f'Something went wrong', VERBOSE['augmenting'])\n",
    "                continue\n",
    "\n",
    "    #X_out, y_out = torch.zeros(N_TRAIN, 1, SIZE['final'], SIZE['final']), torch.zeros(N_TRAIN)\n",
    "    X_out, y_out = torch.zeros(N_TRAIN, 3, SIZE['raw'], SIZE['raw']), torch.zeros(N_TRAIN) #change to work with my data formatting\n",
    "    f = POS_NEG_RATIO / (1 + POS_NEG_RATIO)\n",
    "    for i in range(N_TRAIN):\n",
    "        label = int(torch.rand(1, 1) > f)\n",
    "        X = X_in[y_in == label]\n",
    "        print(X.size)\n",
    "        print(X.shape)\n",
    "        choice = int(torch.randint(0, X.shape[0], (1, 1))) #had to change to X.shape[0] instead of X.size(0)\n",
    "        print(X[choice])\n",
    "        im = torchvision.transforms.functional.to_pil_image(X[choice], mode = 'RGB') #had to specify a mode earlier...\n",
    "        print(im)\n",
    "        X_out[i], y_out[i] = transform(im), label\n",
    "    return X_out, y_out\n",
    "\n",
    "# Function to visualize transformed training data\n",
    "def display(X, h=4, w=5):\n",
    "    s = SIZE['final']\n",
    "    im = np.empty((1, h * s, w * s))\n",
    "    idx = np.random.choice(X.size(0), (h, w), replace=False)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            im[:, s * i:s * (i + 1), s * j:s * (j + 1)] = np.array(X[idx[i, j]])\n",
    "    plt.imshow(im.transpose((1, 2, 0)).repeat(3, 2))\n",
    "    plt.title('Augmented training data sample')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 262144)\n"
     ]
    }
   ],
   "source": [
    "np.array(train_imgs).shape\n",
    "\n",
    "X_messy = np.array(train_imgs)\n",
    "X = np.zeros((70,512*512))\n",
    "X2d = np.zeros((70,512,512))\n",
    "for i in range(70):\n",
    "    for j in range(512):\n",
    "        for k in range(512):\n",
    "            X[i,j*512+k] = X_messy[i,1,j,k]\n",
    "            X2d[i,j,k] = X_messy[i,1,j,k]\n",
    "print(X.shape)\n",
    "X = (X + 1) * 127.5\n",
    "X2d = (X + 1) * 127.5\n",
    "#print(X[:,7])\n",
    "        \n",
    "y = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01568556 0.01960695 0.01568556 ... 0.49803928 0.50588205 0.49803928]\n",
      " [0.21045551 0.21045551 0.21045551 ... 0.60392439 0.58038846 0.5647029 ]\n",
      " [0.08234918 0.         0.00392139 ... 0.49411789 0.62745273 0.87058648]\n",
      " ...\n",
      " [0.50980344 0.51372483 0.51764622 ... 0.89411482 0.88627204 0.88627204]\n",
      " [0.57646707 0.57058498 0.5647029  ... 0.70293948 0.69999844 0.69411635]\n",
      " [0.38529173 0.39411485 0.40587902 ... 0.25588587 0.25588587 0.25588587]]\n"
     ]
    }
   ],
   "source": [
    "toShuffle = np.concatenate((X,y[:,np.newaxis]), axis=1)\n",
    "np.random.shuffle(toShuffle)\n",
    "shuffled_y = toShuffle[:,-1] \n",
    "shuffled_X = toShuffle[:, :-1]\n",
    "print(shuffled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = shuffled_X\n",
    "y = shuffled_y\n",
    "\n",
    "# Parameters for data augmentation\n",
    "TRANSFORMATIONS = torchvision.transforms.Compose([\n",
    "    # Horizontal flip with probability 50%\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # Rotate by up to 15 degrees either way\n",
    "    # Translate by ± 2% of image size\n",
    "    # Expand by up to 15%\n",
    "    torchvision.transforms.RandomAffine(degrees=15, translate=(0.02, 0.02), scale=(1, 1.15)),  # rotation +\n",
    "    # Adjust contrast and brightness by up to 40% and 30% respectively\n",
    "    torchvision.transforms.ColorJitter(contrast=0.4, brightness=0.3),\n",
    "    # Should randomly add black boxes, but doesn't seem to work\n",
    "    torchvision.transforms.RandomErasing(p=0.3, scale=(0.01, 0.03), ratio=(1 / 3, 3)),\n",
    "    # Convert back to torch.tensor\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "4718592\n",
      "(6, 512, 512, 3)\n",
      "[[[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.2274482  0.2274482  0.2274482 ]\n",
      "  [0.22352681 0.22352681 0.22352681]\n",
      "  [0.21568403 0.21568403 0.21568403]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.23136958 0.23136958 0.23136958]\n",
      "  [0.23921236 0.23921236 0.23921236]\n",
      "  [0.2274482  0.2274482  0.2274482 ]]\n",
      "\n",
      " [[0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  [0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.24705514 0.24705514 0.24705514]\n",
      "  [0.25098413 0.25098413 0.25098413]\n",
      "  [0.24313375 0.24313375 0.24313375]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.99999994 0.99999994 0.99999994]\n",
      "  [0.99999994 0.99999994 0.99999994]\n",
      "  [0.99999994 0.99999994 0.99999994]\n",
      "  ...\n",
      "  [0.99215716 0.99215716 0.99215716]\n",
      "  [0.98823577 0.98823577 0.98823577]\n",
      "  [0.98039299 0.98039299 0.98039299]]\n",
      "\n",
      " [[0.99999994 0.99999994 0.99999994]\n",
      "  [0.99999994 0.99999994 0.99999994]\n",
      "  [0.99999994 0.99999994 0.99999994]\n",
      "  ...\n",
      "  [0.98823577 0.98823577 0.98823577]\n",
      "  [0.98431438 0.98431438 0.98431438]\n",
      "  [0.98823577 0.98823577 0.98823577]]\n",
      "\n",
      " [[0.99999994 0.99999994 0.99999994]\n",
      "  [0.99999994 0.99999994 0.99999994]\n",
      "  [0.99999994 0.99999994 0.99999994]\n",
      "  ...\n",
      "  [0.98823577 0.98823577 0.98823577]\n",
      "  [0.98039299 0.98039299 0.98039299]\n",
      "  [0.98823577 0.98823577 0.98823577]]]\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x252183C5A08>\n",
      "4718592\n",
      "(6, 512, 512, 3)\n",
      "[[[0.7882373  0.7882373  0.7882373 ]\n",
      "  [0.98431438 0.98431438 0.98431438]\n",
      "  [0.44313982 0.44313982 0.44313982]\n",
      "  ...\n",
      "  [0.00392139 0.00392139 0.00392139]\n",
      "  [0.00392139 0.00392139 0.00392139]\n",
      "  [0.00392139 0.00392139 0.00392139]]\n",
      "\n",
      " [[0.96078604 0.96078604 0.96078604]\n",
      "  [0.80000147 0.80000147 0.80000147]\n",
      "  [0.17254874 0.17254874 0.17254874]\n",
      "  ...\n",
      "  [0.00392139 0.00392139 0.00392139]\n",
      "  [0.00392139 0.00392139 0.00392139]\n",
      "  [0.00392139 0.00392139 0.00392139]]\n",
      "\n",
      " [[0.68235219 0.68235219 0.68235219]\n",
      "  [0.21176264 0.21176264 0.21176264]\n",
      "  [0.09019956 0.09019956 0.09019956]\n",
      "  ...\n",
      "  [0.00784278 0.00784278 0.00784278]\n",
      "  [0.00784278 0.00784278 0.00784278]\n",
      "  [0.00784278 0.00784278 0.00784278]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.40783972 0.40783972 0.40783972]\n",
      "  [0.41961148 0.41961148 0.41961148]\n",
      "  [0.41961148 0.41961148 0.41961148]\n",
      "  ...\n",
      "  [0.06666362 0.06666362 0.06666362]\n",
      "  [0.07842779 0.07842779 0.07842779]\n",
      "  [0.09019956 0.09019956 0.09019956]]\n",
      "\n",
      " [[0.41568249 0.41568249 0.41568249]\n",
      "  [0.41568249 0.41568249 0.41568249]\n",
      "  [0.42353287 0.42353287 0.42353287]\n",
      "  ...\n",
      "  [0.06666362 0.06666362 0.06666362]\n",
      "  [0.08234918 0.08234918 0.08234918]\n",
      "  [0.09412095 0.09412095 0.09412095]]\n",
      "\n",
      " [[0.30980498 0.30980498 0.30980498]\n",
      "  [0.30980498 0.30980498 0.30980498]\n",
      "  [0.31764776 0.31764776 0.31764776]\n",
      "  ...\n",
      "  [0.05097806 0.05097806 0.05097806]\n",
      "  [0.06274223 0.06274223 0.06274223]\n",
      "  [0.07058501 0.07058501 0.07058501]]]\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x25218183488>\n",
      "4718592\n",
      "(6, 512, 512, 3)\n",
      "[[[0.38038999 0.38038999 0.38038999]\n",
      "  [0.37254721 0.37254721 0.37254721]\n",
      "  [0.36862582 0.36862582 0.36862582]\n",
      "  ...\n",
      "  [0.69803774 0.69803774 0.69803774]\n",
      "  [0.68235219 0.68235219 0.68235219]\n",
      "  [0.66666663 0.66666663 0.66666663]]\n",
      "\n",
      " [[0.39215416 0.39215416 0.39215416]\n",
      "  [0.38823277 0.38823277 0.38823277]\n",
      "  [0.38038999 0.38038999 0.38038999]\n",
      "  ...\n",
      "  [0.6392169  0.6392169  0.6392169 ]\n",
      "  [0.62353134 0.62353134 0.62353134]\n",
      "  [0.61176717 0.61176717 0.61176717]]\n",
      "\n",
      " [[0.39607555 0.39607555 0.39607555]\n",
      "  [0.40783972 0.40783972 0.40783972]\n",
      "  [0.39999694 0.39999694 0.39999694]\n",
      "  ...\n",
      "  [0.56078151 0.56078151 0.56078151]\n",
      "  [0.55293873 0.55293873 0.55293873]\n",
      "  [0.5647029  0.5647029  0.5647029 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.92549354 0.92549354 0.92549354]\n",
      "  [0.93333632 0.93333632 0.93333632]\n",
      "  [0.93725771 0.93725771 0.93725771]\n",
      "  ...\n",
      "  [0.98039299 0.98039299 0.98039299]\n",
      "  [0.98039299 0.98039299 0.98039299]\n",
      "  [0.9764716  0.9764716  0.9764716 ]]\n",
      "\n",
      " [[0.93333632 0.93333632 0.93333632]\n",
      "  [0.9411791  0.9411791  0.9411791 ]\n",
      "  [0.9411791  0.9411791  0.9411791 ]\n",
      "  ...\n",
      "  [0.98039299 0.98039299 0.98039299]\n",
      "  [0.98039299 0.98039299 0.98039299]\n",
      "  [0.98039299 0.98039299 0.98039299]]\n",
      "\n",
      " [[0.9411791  0.9411791  0.9411791 ]\n",
      "  [0.94510049 0.94510049 0.94510049]\n",
      "  [0.9411791  0.9411791  0.9411791 ]\n",
      "  ...\n",
      "  [0.98431438 0.98431438 0.98431438]\n",
      "  [0.97255021 0.97255021 0.97255021]\n",
      "  [0.97255021 0.97255021 0.97255021]]]\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2524F409208>\n",
      "22806528\n",
      "(29, 512, 512, 3)\n",
      "[[[0.05097806 0.05097806 0.05097806]\n",
      "  [0.04705667 0.04705667 0.04705667]\n",
      "  [0.04705667 0.04705667 0.04705667]\n",
      "  ...\n",
      "  [0.04313529 0.04313529 0.04313529]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.08234918 0.08234918 0.08234918]]\n",
      "\n",
      " [[0.04705667 0.04705667 0.04705667]\n",
      "  [0.04705667 0.04705667 0.04705667]\n",
      "  [0.05097806 0.05097806 0.05097806]\n",
      "  ...\n",
      "  [0.04313529 0.04313529 0.04313529]\n",
      "  [0.04313529 0.04313529 0.04313529]\n",
      "  [0.08627817 0.08627817 0.08627817]]\n",
      "\n",
      " [[0.04705667 0.04705667 0.04705667]\n",
      "  [0.05097806 0.05097806 0.05097806]\n",
      "  [0.04705667 0.04705667 0.04705667]\n",
      "  ...\n",
      "  [0.04313529 0.04313529 0.04313529]\n",
      "  [0.04313529 0.04313529 0.04313529]\n",
      "  [0.08234918 0.08234918 0.08234918]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  ...\n",
      "  [0.04313529 0.04313529 0.04313529]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.08234918 0.08234918 0.08234918]]\n",
      "\n",
      " [[0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  ...\n",
      "  [0.04705667 0.04705667 0.04705667]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.08234918 0.08234918 0.08234918]]\n",
      "\n",
      " [[0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  ...\n",
      "  [0.04313529 0.04313529 0.04313529]\n",
      "  [0.0392139  0.0392139  0.0392139 ]\n",
      "  [0.08234918 0.08234918 0.08234918]]]\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x252183C19C8>\n",
      "22806528\n",
      "(29, 512, 512, 3)\n",
      "[[[0.43921843 0.43921843 0.43921843]\n",
      "  [0.43921843 0.43921843 0.43921843]\n",
      "  [0.43921843 0.43921843 0.43921843]\n",
      "  ...\n",
      "  [0.41568249 0.41568249 0.41568249]\n",
      "  [0.41568249 0.41568249 0.41568249]\n",
      "  [0.41176111 0.41176111 0.41176111]]\n",
      "\n",
      " [[0.43921843 0.43921843 0.43921843]\n",
      "  [0.43921843 0.43921843 0.43921843]\n",
      "  [0.43921843 0.43921843 0.43921843]\n",
      "  ...\n",
      "  [0.41568249 0.41568249 0.41568249]\n",
      "  [0.41568249 0.41568249 0.41568249]\n",
      "  [0.41568249 0.41568249 0.41568249]]\n",
      "\n",
      " [[0.43921843 0.43921843 0.43921843]\n",
      "  [0.43921843 0.43921843 0.43921843]\n",
      "  [0.43921843 0.43921843 0.43921843]\n",
      "  ...\n",
      "  [0.41961148 0.41961148 0.41961148]\n",
      "  [0.41961148 0.41961148 0.41961148]\n",
      "  [0.41568249 0.41568249 0.41568249]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.74117303 0.74117303 0.74117303]\n",
      "  [0.74117303 0.74117303 0.74117303]\n",
      "  [0.74117303 0.74117303 0.74117303]\n",
      "  ...\n",
      "  [0.6392169  0.6392169  0.6392169 ]\n",
      "  [0.64313829 0.64313829 0.64313829]\n",
      "  [0.64313829 0.64313829 0.64313829]]\n",
      "\n",
      " [[0.74117303 0.74117303 0.74117303]\n",
      "  [0.74117303 0.74117303 0.74117303]\n",
      "  [0.74117303 0.74117303 0.74117303]\n",
      "  ...\n",
      "  [0.6392169  0.6392169  0.6392169 ]\n",
      "  [0.64313829 0.64313829 0.64313829]\n",
      "  [0.64313829 0.64313829 0.64313829]]\n",
      "\n",
      " [[0.74117303 0.74117303 0.74117303]\n",
      "  [0.74117303 0.74117303 0.74117303]\n",
      "  [0.74117303 0.74117303 0.74117303]\n",
      "  ...\n",
      "  [0.6392169  0.6392169  0.6392169 ]\n",
      "  [0.64313829 0.64313829 0.64313829]\n",
      "  [0.64313829 0.64313829 0.64313829]]]\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x25218183AC8>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'verbose_print' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-390ffa5d68b5>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(im_)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1298\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mget_params\u001b[1;34m(img, scale, ratio, value)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \"\"\"\n\u001b[1;32m-> 1266\u001b[1;33m         \u001b[0mimg_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m         \u001b[0marea\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_h\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Image' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-f84578aa213a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m                         \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRANSFORMATIONS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m#Xtrain, ytrain = augment(Xtrain, ytrain, TRANSFORMATIONS)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-390ffa5d68b5>\u001b[0m in \u001b[0;36maugment\u001b[1;34m(X_in, y_in, transform_fn)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mX_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-390ffa5d68b5>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(im_)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mverbose_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Something went wrong'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVERBOSE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'augmenting'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'verbose_print' is not defined"
     ]
    }
   ],
   "source": [
    "a = np.arange(70)\n",
    "for maxDepth in range(1, 5):\n",
    "    #define classifier here\n",
    "    clf = RandomForestClassifier(max_depth = maxDepth, random_state = 0)\n",
    "    #initialize true positive/false negative/false positive counts (define f1 score)\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    folds =2\n",
    "    for fold in range(folds):\n",
    "        Xtrain = X[a%folds != fold, :]\n",
    "        ytrain = y[a%folds != fold]\n",
    "        Xval = X[a%folds == fold, :]\n",
    "        yval = y[a%folds == fold]\n",
    "        print(ytrain)\n",
    "        #in order to get augment to work...\n",
    "        imgs = np.zeros((Xtrain.shape[0],512,512,3)) #will be Xtrain, but I wanted this to run faster\n",
    "        for idx in range(Xtrain.shape[0]):\n",
    "            obs = np.reshape(Xtrain[idx],(512,512))\n",
    "            img = np.zeros((512,512,3))\n",
    "            for i in range(512):\n",
    "                for j in range(512):\n",
    "                    for k in range(3):                        \n",
    "                        img[i,j,:] = obs[i,j]\n",
    "            imgs[idx,:,:,:] = img\n",
    "        Xtrain, ytrain = augment(imgs, ytrain, TRANSFORMATIONS) #\n",
    "        #Xtrain, ytrain = augment(Xtrain, ytrain, TRANSFORMATIONS)\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        if folds == 70:\n",
    "            preds = clf.predict(Xtest[np.newaxis(), :]) #np.newaxis might be unique to folds 70\n",
    "        else:\n",
    "            preds = clf.predict(Xtest)\n",
    "        #print(str(pred[0]) + \",\"+str(y[holdout]))\n",
    "        for i in range (len(pred)):\n",
    "            if (pred[i] == 1 and yval[i] == 1):\n",
    "                tp += 1\n",
    "            else:\n",
    "                if((pred[i] == 1) and (yval[i] == 0)):\n",
    "                    fp += 1\n",
    "                else:\n",
    "                    if((pred[i] == 0) and (yval[i] == 1)):\n",
    "                        fn += 1\n",
    "    p = tp/(tp+fn)\n",
    "    r = tp/(tp+fp)\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    print(\"MaxDepth: \"+ str(maxDepth) + \", errorrate = \"+ str((fp+fn)/70) + \", f1 = \" + str(f1) + \", p = \" + str(p) + \", r = \" + str(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convolutionForests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a0b1365c0f41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvolutionForests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxDepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxDepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#clf = RandomForestClassifier(max_depth = maxDepth, random_state=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mfolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'convolutionForests' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = shuffled_X\n",
    "y = shuffled_y\n",
    "\n",
    "a = np.arange(70)\n",
    "for maxDepth in range(1, 5):\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    clf = convolutionForests(maxDepth = maxDepth)\n",
    "    #clf = RandomForestClassifier(max_depth = maxDepth, random_state=0)\n",
    "    folds = 70\n",
    "    for fold in range(folds):\n",
    "        clf.fit(X[a%folds != fold, :], y[a%folds != fold])\n",
    "        pred = clf.predict(X[np.newaxis, a[a%folds == fold], :]) #np.newaxis might be unique to folds 70\n",
    "        #print(str(pred[0]) + \",\"+str(y[holdout]))\n",
    "        if (pred[0] == 1 and y[holdout] == 1):\n",
    "            tp += 1 \n",
    "        else:\n",
    "            if((pred[0] == 1) and (y[holdout] == 0)):\n",
    "                fp += 1\n",
    "            else:\n",
    "                if((pred[0] == 0) and (y[holdout] == 1)):\n",
    "                    fn += 1\n",
    "    p = tp/(tp+fn)\n",
    "    r = tp/(tp+fp)\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    print(\"MaxDepth: \"+ str(maxDepth) + \", errorrate = \"+ str((fp+fn)/70) + \", f1 = \" + str(f1) + \", p = \" + str(p) + \", r = \" + str(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class convolutionForests:\n",
    "    \n",
    "    def __init__(self, maxDepth = 3):\n",
    "        self.model = RandomForestClassifier(max_depth = maxDepth, random_state=0)\n",
    "    def fit(self,X,y):\n",
    "        self.convolve(X,y)\n",
    "        self.model.fit(X,y)\n",
    "    \n",
    "    def convolve(self,X,y):\n",
    "        #shifts:\n",
    "        for shift in np.arange(1,4):\n",
    "            Xshifted = np.zeros(X.shape)\n",
    "            for row in range(X.shape[0]):\n",
    "                Xshifted[row, shift:X.shape[1]] = X[row, 0:-1*shift]\n",
    "            X = np.concatenate((Xshifted, X), axis = 0)\n",
    "            y = np.concatenate((y,y), axis=0)\n",
    "        #print(y)\n",
    "    def predict(self, Xtest):\n",
    "        return self.model.predict(Xtest)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
